{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP - `Auto_MPG_data` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gdown --id 1qiUDDoYyRLBiKOoYWdFl_5WByHE8Cugu -O \"data/Auto_MPG_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sate = 59\n",
    "np.random.seed(random_sate)\n",
    "torch.manual_seed(random_sate)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_sate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Japan</th>\n",
       "      <th>USA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0   \n",
       "1  15.0          8         350.0       165.0  3693.0          11.5   \n",
       "2  18.0          8         318.0       150.0  3436.0          11.0   \n",
       "3  16.0          8         304.0       150.0  3433.0          12.0   \n",
       "4  17.0          8         302.0       140.0  3449.0          10.5   \n",
       "\n",
       "   Model Year  Europe  Japan  USA  \n",
       "0          70       0      0    1  \n",
       "1          70       0      0    1  \n",
       "2          70       0      0    1  \n",
       "3          70       0      0    1  \n",
       "4          70       0      0    1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/Auto_MPG_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((392, 9), (392,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:, 1:].values\n",
    "y = df[\"MPG\"].values\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((273, 9), (79, 9), (40, 9), (273,), (79,), (40,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=random_sate,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.125,\n",
    "    random_state=random_sate,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = StandardScaler()\n",
    "\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_val = normalizer.transform(X_val)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y, y_pred):\n",
    "    y = torch.Tensor(y).to(device)\n",
    "    y_pred = torch.Tensor(y_pred).to(device)\n",
    "    mean_true = torch.mean(y)\n",
    "    ss_tot = torch.sum((y - mean_true) ** 2)\n",
    "    ss_res = torch.sum((y - y_pred) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.linear2 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.output = nn.Linear(hidden_dims, output_dims)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        out = self.output(x)\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (linear1): Linear(in_features=9, out_features=64, bias=True)\n",
       "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(input_dims=X_train.shape[1], hidden_dims=64, output_dims=1).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1: \tTraining loss: 396.203 \tValidation loss: 172.711\n",
      "\n",
      " Epoch 2: \tTraining loss: 149.594 \tValidation loss: 44.465\n",
      "\n",
      " Epoch 3: \tTraining loss: 121.731 \tValidation loss: 12.565\n",
      "\n",
      " Epoch 4: \tTraining loss: 18.505 \tValidation loss: 280.319\n",
      "\n",
      " Epoch 5: \tTraining loss: 163.226 \tValidation loss: 34.213\n",
      "\n",
      " Epoch 6: \tTraining loss: 20.306 \tValidation loss: 11.482\n",
      "\n",
      " Epoch 7: \tTraining loss: 12.106 \tValidation loss: 11.638\n",
      "\n",
      " Epoch 8: \tTraining loss: 13.820 \tValidation loss: 16.028\n",
      "\n",
      " Epoch 9: \tTraining loss: 13.846 \tValidation loss: 7.095\n",
      "\n",
      " Epoch 10: \tTraining loss: 15.788 \tValidation loss: 9.309\n",
      "\n",
      " Epoch 11: \tTraining loss: 11.958 \tValidation loss: 14.595\n",
      "\n",
      " Epoch 12: \tTraining loss: 10.753 \tValidation loss: 7.530\n",
      "\n",
      " Epoch 13: \tTraining loss: 7.541 \tValidation loss: 10.285\n",
      "\n",
      " Epoch 14: \tTraining loss: 14.035 \tValidation loss: 7.154\n",
      "\n",
      " Epoch 15: \tTraining loss: 7.222 \tValidation loss: 6.249\n",
      "\n",
      " Epoch 16: \tTraining loss: 6.965 \tValidation loss: 5.568\n",
      "\n",
      " Epoch 17: \tTraining loss: 10.797 \tValidation loss: 5.696\n",
      "\n",
      " Epoch 18: \tTraining loss: 7.555 \tValidation loss: 9.641\n",
      "\n",
      " Epoch 19: \tTraining loss: 8.531 \tValidation loss: 5.522\n",
      "\n",
      " Epoch 20: \tTraining loss: 7.662 \tValidation loss: 7.496\n",
      "\n",
      " Epoch 21: \tTraining loss: 7.256 \tValidation loss: 7.090\n",
      "\n",
      " Epoch 22: \tTraining loss: 7.742 \tValidation loss: 10.480\n",
      "\n",
      " Epoch 23: \tTraining loss: 6.904 \tValidation loss: 5.476\n",
      "\n",
      " Epoch 24: \tTraining loss: 5.946 \tValidation loss: 14.026\n",
      "\n",
      " Epoch 25: \tTraining loss: 7.089 \tValidation loss: 6.701\n",
      "\n",
      " Epoch 26: \tTraining loss: 8.487 \tValidation loss: 6.428\n",
      "\n",
      " Epoch 27: \tTraining loss: 7.306 \tValidation loss: 16.825\n",
      "\n",
      " Epoch 28: \tTraining loss: 9.635 \tValidation loss: 13.359\n",
      "\n",
      " Epoch 29: \tTraining loss: 7.486 \tValidation loss: 5.605\n",
      "\n",
      " Epoch 30: \tTraining loss: 6.332 \tValidation loss: 5.071\n",
      "\n",
      " Epoch 31: \tTraining loss: 6.669 \tValidation loss: 7.306\n",
      "\n",
      " Epoch 32: \tTraining loss: 7.837 \tValidation loss: 23.116\n",
      "\n",
      " Epoch 33: \tTraining loss: 8.492 \tValidation loss: 8.966\n",
      "\n",
      " Epoch 34: \tTraining loss: 6.310 \tValidation loss: 7.082\n",
      "\n",
      " Epoch 35: \tTraining loss: 8.360 \tValidation loss: 10.143\n",
      "\n",
      " Epoch 36: \tTraining loss: 6.683 \tValidation loss: 5.192\n",
      "\n",
      " Epoch 37: \tTraining loss: 5.963 \tValidation loss: 7.545\n",
      "\n",
      " Epoch 38: \tTraining loss: 6.672 \tValidation loss: 5.092\n",
      "\n",
      " Epoch 39: \tTraining loss: 5.799 \tValidation loss: 5.387\n",
      "\n",
      " Epoch 40: \tTraining loss: 8.976 \tValidation loss: 31.948\n",
      "\n",
      " Epoch 41: \tTraining loss: 10.268 \tValidation loss: 10.663\n",
      "\n",
      " Epoch 42: \tTraining loss: 10.431 \tValidation loss: 5.306\n",
      "\n",
      " Epoch 43: \tTraining loss: 5.831 \tValidation loss: 5.132\n",
      "\n",
      " Epoch 44: \tTraining loss: 6.353 \tValidation loss: 4.629\n",
      "\n",
      " Epoch 45: \tTraining loss: 7.386 \tValidation loss: 7.682\n",
      "\n",
      " Epoch 46: \tTraining loss: 7.107 \tValidation loss: 6.415\n",
      "\n",
      " Epoch 47: \tTraining loss: 7.981 \tValidation loss: 5.263\n",
      "\n",
      " Epoch 48: \tTraining loss: 5.759 \tValidation loss: 5.359\n",
      "\n",
      " Epoch 49: \tTraining loss: 5.633 \tValidation loss: 6.295\n",
      "\n",
      " Epoch 50: \tTraining loss: 7.036 \tValidation loss: 5.067\n",
      "\n",
      " Epoch 51: \tTraining loss: 5.777 \tValidation loss: 5.682\n",
      "\n",
      " Epoch 52: \tTraining loss: 5.589 \tValidation loss: 4.779\n",
      "\n",
      " Epoch 53: \tTraining loss: 6.722 \tValidation loss: 8.860\n",
      "\n",
      " Epoch 54: \tTraining loss: 6.748 \tValidation loss: 6.562\n",
      "\n",
      " Epoch 55: \tTraining loss: 5.998 \tValidation loss: 6.262\n",
      "\n",
      " Epoch 56: \tTraining loss: 5.418 \tValidation loss: 5.282\n",
      "\n",
      " Epoch 57: \tTraining loss: 6.089 \tValidation loss: 7.175\n",
      "\n",
      " Epoch 58: \tTraining loss: 5.659 \tValidation loss: 7.501\n",
      "\n",
      " Epoch 59: \tTraining loss: 7.724 \tValidation loss: 7.174\n",
      "\n",
      " Epoch 60: \tTraining loss: 7.061 \tValidation loss: 6.212\n",
      "\n",
      " Epoch 61: \tTraining loss: 6.975 \tValidation loss: 8.736\n",
      "\n",
      " Epoch 62: \tTraining loss: 6.104 \tValidation loss: 7.273\n",
      "\n",
      " Epoch 63: \tTraining loss: 6.240 \tValidation loss: 12.907\n",
      "\n",
      " Epoch 64: \tTraining loss: 8.281 \tValidation loss: 5.794\n",
      "\n",
      " Epoch 65: \tTraining loss: 5.565 \tValidation loss: 4.441\n",
      "\n",
      " Epoch 66: \tTraining loss: 5.827 \tValidation loss: 10.269\n",
      "\n",
      " Epoch 67: \tTraining loss: 6.536 \tValidation loss: 4.901\n",
      "\n",
      " Epoch 68: \tTraining loss: 6.556 \tValidation loss: 6.267\n",
      "\n",
      " Epoch 69: \tTraining loss: 5.079 \tValidation loss: 7.384\n",
      "\n",
      " Epoch 70: \tTraining loss: 9.351 \tValidation loss: 5.277\n",
      "\n",
      " Epoch 71: \tTraining loss: 8.949 \tValidation loss: 8.495\n",
      "\n",
      " Epoch 72: \tTraining loss: 7.462 \tValidation loss: 11.969\n",
      "\n",
      " Epoch 73: \tTraining loss: 8.025 \tValidation loss: 4.526\n",
      "\n",
      " Epoch 74: \tTraining loss: 5.660 \tValidation loss: 5.553\n",
      "\n",
      " Epoch 75: \tTraining loss: 5.629 \tValidation loss: 5.614\n",
      "\n",
      " Epoch 76: \tTraining loss: 5.108 \tValidation loss: 5.780\n",
      "\n",
      " Epoch 77: \tTraining loss: 5.556 \tValidation loss: 6.351\n",
      "\n",
      " Epoch 78: \tTraining loss: 6.541 \tValidation loss: 6.519\n",
      "\n",
      " Epoch 79: \tTraining loss: 5.759 \tValidation loss: 10.894\n",
      "\n",
      " Epoch 80: \tTraining loss: 7.265 \tValidation loss: 12.153\n",
      "\n",
      " Epoch 81: \tTraining loss: 5.920 \tValidation loss: 7.065\n",
      "\n",
      " Epoch 82: \tTraining loss: 4.738 \tValidation loss: 5.581\n",
      "\n",
      " Epoch 83: \tTraining loss: 5.351 \tValidation loss: 5.349\n",
      "\n",
      " Epoch 84: \tTraining loss: 5.094 \tValidation loss: 15.374\n",
      "\n",
      " Epoch 85: \tTraining loss: 5.973 \tValidation loss: 6.396\n",
      "\n",
      " Epoch 86: \tTraining loss: 7.675 \tValidation loss: 13.071\n",
      "\n",
      " Epoch 87: \tTraining loss: 6.384 \tValidation loss: 23.012\n",
      "\n",
      " Epoch 88: \tTraining loss: 8.928 \tValidation loss: 5.204\n",
      "\n",
      " Epoch 89: \tTraining loss: 4.657 \tValidation loss: 5.537\n",
      "\n",
      " Epoch 90: \tTraining loss: 4.922 \tValidation loss: 7.593\n",
      "\n",
      " Epoch 91: \tTraining loss: 4.921 \tValidation loss: 4.792\n",
      "\n",
      " Epoch 92: \tTraining loss: 4.780 \tValidation loss: 4.671\n",
      "\n",
      " Epoch 93: \tTraining loss: 4.935 \tValidation loss: 5.428\n",
      "\n",
      " Epoch 94: \tTraining loss: 5.128 \tValidation loss: 4.752\n",
      "\n",
      " Epoch 95: \tTraining loss: 6.438 \tValidation loss: 6.200\n",
      "\n",
      " Epoch 96: \tTraining loss: 4.941 \tValidation loss: 6.546\n",
      "\n",
      " Epoch 97: \tTraining loss: 4.758 \tValidation loss: 5.927\n",
      "\n",
      " Epoch 98: \tTraining loss: 4.805 \tValidation loss: 5.092\n",
      "\n",
      " Epoch 99: \tTraining loss: 4.406 \tValidation loss: 7.615\n",
      "\n",
      " Epoch 100: \tTraining loss: 5.886 \tValidation loss: 7.015\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "train_losses, val_losses = [], []\n",
    "train_r2, val_r2 = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    train_target, train_predict = [], []\n",
    "    val_target, val_predict = [], []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for X_samples, y_samples in train_loader:\n",
    "        X_samples = X_samples.to(device)\n",
    "        y_samples = y_samples.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(X_samples)\n",
    "        loss = criterion(outputs, y_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_predict += outputs.tolist()\n",
    "        train_target += y_samples.tolist()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_r2.append(r_squared(train_target, train_predict))\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_samples, y_samples in val_loader:\n",
    "            X_samples = X_samples.to(device)\n",
    "            y_samples = y_samples.to(device)\n",
    "            \n",
    "            outputs = model(X_samples)\n",
    "            loss = criterion(outputs, y_samples)\n",
    "            \n",
    "            val_predict += outputs.tolist()\n",
    "            val_target += y_samples.tolist()\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_r2.append(r_squared(val_target, val_predict))\n",
    "    print(f\"\\n Epoch {epoch + 1}: \\tTraining loss: {train_loss:.3f} \\tValidation loss: {val_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test set\n",
      "R2: 0.8743938207626343\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test.to(device))\n",
    "    test_set_r2 = r_squared(y_test, y_pred)\n",
    "    print(\"Evaluation on test set\")\n",
    "    print(f\"R2: {test_set_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 6:\n",
      "x = 5.1 - Probability: 0.827 - Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor([[5.1], [6.0], [5.7]], dtype=torch.float32)\n",
    "y_train = torch.tensor([[0], [1], [1]], dtype=torch.float32)\n",
    "\n",
    "class FlowersMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.hidden.weight = nn.Parameter(torch.tensor([[0.5], [-0.5]], dtype=torch.float32))\n",
    "            self.hidden.bias = nn.Parameter(torch.tensor([1.0, -1.0], dtype=torch.float32))\n",
    "            self.output.weight = nn.Parameter(torch.tensor([[0.3, -0.2]], dtype=torch.float32))\n",
    "            self.output.bias = nn.Parameter(torch.tensor([0.5], dtype=torch.float32))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "model = FlowersMLP()\n",
    "\n",
    "x_test = torch.tensor([[5.1]])\n",
    "with torch.no_grad():\n",
    "    output = model(x_test)\n",
    "    prediction = 1 if output.item() > 0.5 else 0\n",
    "\n",
    "print(\"Question 6:\")\n",
    "print(f\"x = 5.1 - Probability: {output.item():.3f} - Predicted class: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 7:\n",
      "x = 50: 39.00\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor([[50], [60], [70]], dtype=torch.float32)\n",
    "y_train = torch.tensor([[100], [120], [140]], dtype=torch.float32)\n",
    "\n",
    "class HousePriceMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(2, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.hidden.weight = nn.Parameter(torch.tensor([[0.5], [-0.5]], dtype=torch.float32))\n",
    "            self.hidden.bias = nn.Parameter(torch.tensor([1.0, -1.0], dtype=torch.float32))\n",
    "            self.output.weight = nn.Parameter(torch.tensor([[1.5, 2.0]], dtype=torch.float32))\n",
    "            self.output.bias = nn.Parameter(torch.tensor([0.0], dtype=torch.float32))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        return self.output(x)\n",
    "\n",
    "model = HousePriceMLP()\n",
    "x_input = torch.tensor([[50.0]])\n",
    "with torch.no_grad():\n",
    "    predicted_price = model(x_input)\n",
    "\n",
    "print(\"Question 7:\")\n",
    "print(f\"x = 50: {predicted_price.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 8.1:\n",
      "\n",
      " Epoch 1: \tTraining loss: 533.760 \tValidation loss: 439.767\n",
      "\n",
      " Epoch 2: \tTraining loss: 358.632 \tValidation loss: 313.511\n",
      "\n",
      " Epoch 3: \tTraining loss: 249.830 \tValidation loss: 225.462\n",
      "\n",
      " Epoch 4: \tTraining loss: 178.998 \tValidation loss: 164.626\n",
      "\n",
      " Epoch 5: \tTraining loss: 128.302 \tValidation loss: 119.231\n",
      "\n",
      " Epoch 6: \tTraining loss: 90.892 \tValidation loss: 88.113\n",
      "\n",
      " Epoch 7: \tTraining loss: 69.484 \tValidation loss: 65.751\n",
      "\n",
      " Epoch 8: \tTraining loss: 50.795 \tValidation loss: 50.193\n",
      "\n",
      " Epoch 9: \tTraining loss: 39.932 \tValidation loss: 38.682\n",
      "\n",
      " Epoch 10: \tTraining loss: 31.264 \tValidation loss: 30.997\n",
      "\n",
      " Epoch 11: \tTraining loss: 27.062 \tValidation loss: 25.312\n",
      "\n",
      " Epoch 12: \tTraining loss: 22.848 \tValidation loss: 20.277\n",
      "\n",
      " Epoch 13: \tTraining loss: 19.968 \tValidation loss: 17.572\n",
      "\n",
      " Epoch 14: \tTraining loss: 16.891 \tValidation loss: 15.549\n",
      "\n",
      " Epoch 15: \tTraining loss: 15.496 \tValidation loss: 13.792\n",
      "\n",
      " Epoch 16: \tTraining loss: 14.605 \tValidation loss: 12.617\n",
      "\n",
      " Epoch 17: \tTraining loss: 14.469 \tValidation loss: 11.973\n",
      "\n",
      " Epoch 18: \tTraining loss: 14.560 \tValidation loss: 11.498\n",
      "\n",
      " Epoch 19: \tTraining loss: 12.950 \tValidation loss: 10.615\n",
      "\n",
      " Epoch 20: \tTraining loss: 13.728 \tValidation loss: 10.365\n",
      "\n",
      " Epoch 21: \tTraining loss: 13.041 \tValidation loss: 9.799\n",
      "\n",
      " Epoch 22: \tTraining loss: 12.919 \tValidation loss: 9.793\n",
      "\n",
      " Epoch 23: \tTraining loss: 12.839 \tValidation loss: 9.488\n",
      "\n",
      " Epoch 24: \tTraining loss: 12.616 \tValidation loss: 9.242\n",
      "\n",
      " Epoch 25: \tTraining loss: 12.859 \tValidation loss: 9.085\n",
      "\n",
      " Epoch 26: \tTraining loss: 12.363 \tValidation loss: 9.146\n",
      "\n",
      " Epoch 27: \tTraining loss: 12.984 \tValidation loss: 9.238\n",
      "\n",
      " Epoch 28: \tTraining loss: 12.472 \tValidation loss: 9.269\n",
      "\n",
      " Epoch 29: \tTraining loss: 12.396 \tValidation loss: 8.997\n",
      "\n",
      " Epoch 30: \tTraining loss: 12.488 \tValidation loss: 8.871\n",
      "\n",
      " Epoch 31: \tTraining loss: 12.720 \tValidation loss: 8.745\n",
      "\n",
      " Epoch 32: \tTraining loss: 12.030 \tValidation loss: 8.770\n",
      "\n",
      " Epoch 33: \tTraining loss: 12.190 \tValidation loss: 8.600\n",
      "\n",
      " Epoch 34: \tTraining loss: 12.095 \tValidation loss: 8.655\n",
      "\n",
      " Epoch 35: \tTraining loss: 12.615 \tValidation loss: 8.862\n",
      "\n",
      " Epoch 36: \tTraining loss: 13.294 \tValidation loss: 8.975\n",
      "\n",
      " Epoch 37: \tTraining loss: 13.196 \tValidation loss: 9.007\n",
      "\n",
      " Epoch 38: \tTraining loss: 12.188 \tValidation loss: 8.788\n",
      "\n",
      " Epoch 39: \tTraining loss: 12.503 \tValidation loss: 8.608\n",
      "\n",
      " Epoch 40: \tTraining loss: 11.985 \tValidation loss: 8.688\n",
      "\n",
      " Epoch 41: \tTraining loss: 12.122 \tValidation loss: 8.553\n",
      "\n",
      " Epoch 42: \tTraining loss: 12.218 \tValidation loss: 8.575\n",
      "\n",
      " Epoch 43: \tTraining loss: 12.587 \tValidation loss: 8.704\n",
      "\n",
      " Epoch 44: \tTraining loss: 12.729 \tValidation loss: 8.991\n",
      "\n",
      " Epoch 45: \tTraining loss: 12.293 \tValidation loss: 8.683\n",
      "\n",
      " Epoch 46: \tTraining loss: 12.196 \tValidation loss: 8.491\n",
      "\n",
      " Epoch 47: \tTraining loss: 12.740 \tValidation loss: 8.908\n",
      "\n",
      " Epoch 48: \tTraining loss: 12.420 \tValidation loss: 8.764\n",
      "\n",
      " Epoch 49: \tTraining loss: 12.492 \tValidation loss: 8.762\n",
      "\n",
      " Epoch 50: \tTraining loss: 11.917 \tValidation loss: 8.685\n",
      "\n",
      " Epoch 51: \tTraining loss: 12.358 \tValidation loss: 8.605\n",
      "\n",
      " Epoch 52: \tTraining loss: 12.091 \tValidation loss: 8.626\n",
      "\n",
      " Epoch 53: \tTraining loss: 12.588 \tValidation loss: 8.665\n",
      "\n",
      " Epoch 54: \tTraining loss: 12.028 \tValidation loss: 8.631\n",
      "\n",
      " Epoch 55: \tTraining loss: 11.651 \tValidation loss: 8.632\n",
      "\n",
      " Epoch 56: \tTraining loss: 11.724 \tValidation loss: 8.463\n",
      "\n",
      " Epoch 57: \tTraining loss: 12.107 \tValidation loss: 8.307\n",
      "\n",
      " Epoch 58: \tTraining loss: 12.372 \tValidation loss: 8.349\n",
      "\n",
      " Epoch 59: \tTraining loss: 12.238 \tValidation loss: 8.370\n",
      "\n",
      " Epoch 60: \tTraining loss: 11.780 \tValidation loss: 8.418\n",
      "\n",
      " Epoch 61: \tTraining loss: 11.545 \tValidation loss: 8.412\n",
      "\n",
      " Epoch 62: \tTraining loss: 12.391 \tValidation loss: 8.609\n",
      "\n",
      " Epoch 63: \tTraining loss: 12.135 \tValidation loss: 8.287\n",
      "\n",
      " Epoch 64: \tTraining loss: 12.193 \tValidation loss: 8.375\n",
      "\n",
      " Epoch 65: \tTraining loss: 11.686 \tValidation loss: 8.356\n",
      "\n",
      " Epoch 66: \tTraining loss: 12.492 \tValidation loss: 8.576\n",
      "\n",
      " Epoch 67: \tTraining loss: 11.745 \tValidation loss: 8.528\n",
      "\n",
      " Epoch 68: \tTraining loss: 11.862 \tValidation loss: 8.331\n",
      "\n",
      " Epoch 69: \tTraining loss: 11.751 \tValidation loss: 8.422\n",
      "\n",
      " Epoch 70: \tTraining loss: 11.902 \tValidation loss: 8.483\n",
      "\n",
      " Epoch 71: \tTraining loss: 11.942 \tValidation loss: 8.321\n",
      "\n",
      " Epoch 72: \tTraining loss: 12.104 \tValidation loss: 8.264\n",
      "\n",
      " Epoch 73: \tTraining loss: 11.928 \tValidation loss: 8.415\n",
      "\n",
      " Epoch 74: \tTraining loss: 11.767 \tValidation loss: 8.306\n",
      "\n",
      " Epoch 75: \tTraining loss: 11.991 \tValidation loss: 8.226\n",
      "\n",
      " Epoch 76: \tTraining loss: 11.653 \tValidation loss: 8.285\n",
      "\n",
      " Epoch 77: \tTraining loss: 11.916 \tValidation loss: 8.269\n",
      "\n",
      " Epoch 78: \tTraining loss: 12.692 \tValidation loss: 8.484\n",
      "\n",
      " Epoch 79: \tTraining loss: 12.126 \tValidation loss: 8.464\n",
      "\n",
      " Epoch 80: \tTraining loss: 12.221 \tValidation loss: 8.566\n",
      "\n",
      " Epoch 81: \tTraining loss: 11.872 \tValidation loss: 8.406\n",
      "\n",
      " Epoch 82: \tTraining loss: 11.524 \tValidation loss: 8.392\n",
      "\n",
      " Epoch 83: \tTraining loss: 12.326 \tValidation loss: 8.481\n",
      "\n",
      " Epoch 84: \tTraining loss: 11.549 \tValidation loss: 8.509\n",
      "\n",
      " Epoch 85: \tTraining loss: 12.227 \tValidation loss: 8.408\n",
      "\n",
      " Epoch 86: \tTraining loss: 11.474 \tValidation loss: 8.260\n",
      "\n",
      " Epoch 87: \tTraining loss: 11.540 \tValidation loss: 8.346\n",
      "\n",
      " Epoch 88: \tTraining loss: 11.999 \tValidation loss: 8.372\n",
      "\n",
      " Epoch 89: \tTraining loss: 12.479 \tValidation loss: 8.556\n",
      "\n",
      " Epoch 90: \tTraining loss: 11.752 \tValidation loss: 8.289\n",
      "\n",
      " Epoch 91: \tTraining loss: 11.867 \tValidation loss: 8.318\n",
      "\n",
      " Epoch 92: \tTraining loss: 11.993 \tValidation loss: 8.288\n",
      "\n",
      " Epoch 93: \tTraining loss: 11.607 \tValidation loss: 8.249\n",
      "\n",
      " Epoch 94: \tTraining loss: 11.964 \tValidation loss: 8.237\n",
      "\n",
      " Epoch 95: \tTraining loss: 12.126 \tValidation loss: 8.382\n",
      "\n",
      " Epoch 96: \tTraining loss: 11.959 \tValidation loss: 8.367\n",
      "\n",
      " Epoch 97: \tTraining loss: 11.582 \tValidation loss: 8.263\n",
      "\n",
      " Epoch 98: \tTraining loss: 11.747 \tValidation loss: 8.309\n",
      "\n",
      " Epoch 99: \tTraining loss: 11.695 \tValidation loss: 8.209\n",
      "\n",
      " Epoch 100: \tTraining loss: 11.877 \tValidation loss: 8.273\n",
      "Evaluation on test set\n",
      "R2: 0.8280044794082642\n"
     ]
    }
   ],
   "source": [
    "print(\"Question 8.1:\")\n",
    "\n",
    "class MLP81(nn.Module):\n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dims, output_dims)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x).squeeze(1)\n",
    "\n",
    "model = MLP81(X_train.shape[1], 1).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "epochs = 100\n",
    "train_losses, val_losses = [], []\n",
    "train_r2, val_r2 = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    train_target, train_predict = [], []\n",
    "    val_target, val_predict = [], []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for X_samples, y_samples in train_loader:\n",
    "        X_samples = X_samples.to(device)\n",
    "        y_samples = y_samples.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(X_samples)\n",
    "        loss = criterion(outputs, y_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_predict += outputs.tolist()\n",
    "        train_target += y_samples.tolist()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_r2.append(r_squared(train_target, train_predict))\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_samples, y_samples in val_loader:\n",
    "            X_samples = X_samples.to(device)\n",
    "            y_samples = y_samples.to(device)\n",
    "            \n",
    "            outputs = model(X_samples)\n",
    "            loss = criterion(outputs, y_samples)\n",
    "            \n",
    "            val_predict += outputs.tolist()\n",
    "            val_target += y_samples.tolist()\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_r2.append(r_squared(val_target, val_predict))\n",
    "    print(f\"\\n Epoch {epoch + 1}: \\tTraining loss: {train_loss:.3f} \\tValidation loss: {val_loss:.3f}\")\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test.to(device))\n",
    "    test_set_r2 = r_squared(y_test, y_pred)\n",
    "    print(\"Evaluation on test set\")\n",
    "    print(f\"R2: {test_set_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 8.2:\n",
      "\n",
      " Epoch 1: \tTraining loss: 146.833 \tValidation loss: 16.647\n",
      "\n",
      " Epoch 2: \tTraining loss: 20.054 \tValidation loss: 10.036\n",
      "\n",
      " Epoch 3: \tTraining loss: 15.585 \tValidation loss: 9.003\n",
      "\n",
      " Epoch 4: \tTraining loss: 14.345 \tValidation loss: 8.059\n",
      "\n",
      " Epoch 5: \tTraining loss: 13.605 \tValidation loss: 8.060\n",
      "\n",
      " Epoch 6: \tTraining loss: 13.013 \tValidation loss: 7.782\n",
      "\n",
      " Epoch 7: \tTraining loss: 12.795 \tValidation loss: 8.061\n",
      "\n",
      " Epoch 8: \tTraining loss: 13.166 \tValidation loss: 7.405\n",
      "\n",
      " Epoch 9: \tTraining loss: 12.543 \tValidation loss: 7.455\n",
      "\n",
      " Epoch 10: \tTraining loss: 12.424 \tValidation loss: 7.263\n",
      "\n",
      " Epoch 11: \tTraining loss: 11.758 \tValidation loss: 7.511\n",
      "\n",
      " Epoch 12: \tTraining loss: 12.340 \tValidation loss: 7.253\n",
      "\n",
      " Epoch 13: \tTraining loss: 11.682 \tValidation loss: 7.240\n",
      "\n",
      " Epoch 14: \tTraining loss: 12.260 \tValidation loss: 7.325\n",
      "\n",
      " Epoch 15: \tTraining loss: 11.602 \tValidation loss: 7.029\n",
      "\n",
      " Epoch 16: \tTraining loss: 11.281 \tValidation loss: 7.296\n",
      "\n",
      " Epoch 17: \tTraining loss: 11.737 \tValidation loss: 7.633\n",
      "\n",
      " Epoch 18: \tTraining loss: 11.406 \tValidation loss: 6.981\n",
      "\n",
      " Epoch 19: \tTraining loss: 11.233 \tValidation loss: 7.471\n",
      "\n",
      " Epoch 20: \tTraining loss: 11.423 \tValidation loss: 6.668\n",
      "\n",
      " Epoch 21: \tTraining loss: 10.974 \tValidation loss: 6.885\n",
      "\n",
      " Epoch 22: \tTraining loss: 11.723 \tValidation loss: 6.663\n",
      "\n",
      " Epoch 23: \tTraining loss: 10.987 \tValidation loss: 7.280\n",
      "\n",
      " Epoch 24: \tTraining loss: 11.112 \tValidation loss: 6.472\n",
      "\n",
      " Epoch 25: \tTraining loss: 11.034 \tValidation loss: 6.302\n",
      "\n",
      " Epoch 26: \tTraining loss: 10.450 \tValidation loss: 6.402\n",
      "\n",
      " Epoch 27: \tTraining loss: 11.003 \tValidation loss: 6.366\n",
      "\n",
      " Epoch 28: \tTraining loss: 10.799 \tValidation loss: 6.175\n",
      "\n",
      " Epoch 29: \tTraining loss: 10.476 \tValidation loss: 6.251\n",
      "\n",
      " Epoch 30: \tTraining loss: 10.171 \tValidation loss: 6.237\n",
      "\n",
      " Epoch 31: \tTraining loss: 10.547 \tValidation loss: 6.044\n",
      "\n",
      " Epoch 32: \tTraining loss: 10.152 \tValidation loss: 5.937\n",
      "\n",
      " Epoch 33: \tTraining loss: 10.262 \tValidation loss: 7.072\n",
      "\n",
      " Epoch 34: \tTraining loss: 10.022 \tValidation loss: 5.972\n",
      "\n",
      " Epoch 35: \tTraining loss: 10.195 \tValidation loss: 5.800\n",
      "\n",
      " Epoch 36: \tTraining loss: 9.892 \tValidation loss: 5.825\n",
      "\n",
      " Epoch 37: \tTraining loss: 9.847 \tValidation loss: 6.219\n",
      "\n",
      " Epoch 38: \tTraining loss: 10.083 \tValidation loss: 5.657\n",
      "\n",
      " Epoch 39: \tTraining loss: 9.761 \tValidation loss: 6.068\n",
      "\n",
      " Epoch 40: \tTraining loss: 9.545 \tValidation loss: 5.612\n",
      "\n",
      " Epoch 41: \tTraining loss: 9.934 \tValidation loss: 5.500\n",
      "\n",
      " Epoch 42: \tTraining loss: 9.673 \tValidation loss: 5.431\n",
      "\n",
      " Epoch 43: \tTraining loss: 9.541 \tValidation loss: 5.933\n",
      "\n",
      " Epoch 44: \tTraining loss: 9.500 \tValidation loss: 5.764\n",
      "\n",
      " Epoch 45: \tTraining loss: 9.339 \tValidation loss: 5.696\n",
      "\n",
      " Epoch 46: \tTraining loss: 9.119 \tValidation loss: 5.741\n",
      "\n",
      " Epoch 47: \tTraining loss: 9.212 \tValidation loss: 5.410\n",
      "\n",
      " Epoch 48: \tTraining loss: 9.074 \tValidation loss: 5.670\n",
      "\n",
      " Epoch 49: \tTraining loss: 9.124 \tValidation loss: 5.435\n",
      "\n",
      " Epoch 50: \tTraining loss: 9.342 \tValidation loss: 5.313\n",
      "\n",
      " Epoch 51: \tTraining loss: 9.266 \tValidation loss: 5.293\n",
      "\n",
      " Epoch 52: \tTraining loss: 9.249 \tValidation loss: 5.330\n",
      "\n",
      " Epoch 53: \tTraining loss: 9.169 \tValidation loss: 5.287\n",
      "\n",
      " Epoch 54: \tTraining loss: 9.315 \tValidation loss: 5.290\n",
      "\n",
      " Epoch 55: \tTraining loss: 9.300 \tValidation loss: 5.388\n",
      "\n",
      " Epoch 56: \tTraining loss: 9.163 \tValidation loss: 5.224\n",
      "\n",
      " Epoch 57: \tTraining loss: 9.007 \tValidation loss: 5.701\n",
      "\n",
      " Epoch 58: \tTraining loss: 9.022 \tValidation loss: 5.350\n",
      "\n",
      " Epoch 59: \tTraining loss: 8.891 \tValidation loss: 5.457\n",
      "\n",
      " Epoch 60: \tTraining loss: 9.022 \tValidation loss: 5.230\n",
      "\n",
      " Epoch 61: \tTraining loss: 8.816 \tValidation loss: 5.111\n",
      "\n",
      " Epoch 62: \tTraining loss: 9.000 \tValidation loss: 5.151\n",
      "\n",
      " Epoch 63: \tTraining loss: 9.334 \tValidation loss: 5.134\n",
      "\n",
      " Epoch 64: \tTraining loss: 8.581 \tValidation loss: 5.371\n",
      "\n",
      " Epoch 65: \tTraining loss: 8.642 \tValidation loss: 5.276\n",
      "\n",
      " Epoch 66: \tTraining loss: 8.545 \tValidation loss: 5.055\n",
      "\n",
      " Epoch 67: \tTraining loss: 8.823 \tValidation loss: 5.154\n",
      "\n",
      " Epoch 68: \tTraining loss: 8.584 \tValidation loss: 5.261\n",
      "\n",
      " Epoch 69: \tTraining loss: 9.044 \tValidation loss: 5.074\n",
      "\n",
      " Epoch 70: \tTraining loss: 8.587 \tValidation loss: 5.011\n",
      "\n",
      " Epoch 71: \tTraining loss: 8.630 \tValidation loss: 5.184\n",
      "\n",
      " Epoch 72: \tTraining loss: 8.579 \tValidation loss: 5.091\n",
      "\n",
      " Epoch 73: \tTraining loss: 8.359 \tValidation loss: 5.010\n",
      "\n",
      " Epoch 74: \tTraining loss: 8.431 \tValidation loss: 4.979\n",
      "\n",
      " Epoch 75: \tTraining loss: 8.562 \tValidation loss: 5.042\n",
      "\n",
      " Epoch 76: \tTraining loss: 8.746 \tValidation loss: 5.096\n",
      "\n",
      " Epoch 77: \tTraining loss: 8.572 \tValidation loss: 5.143\n",
      "\n",
      " Epoch 78: \tTraining loss: 8.505 \tValidation loss: 5.005\n",
      "\n",
      " Epoch 79: \tTraining loss: 8.294 \tValidation loss: 5.016\n",
      "\n",
      " Epoch 80: \tTraining loss: 8.648 \tValidation loss: 5.006\n",
      "\n",
      " Epoch 81: \tTraining loss: 8.292 \tValidation loss: 4.985\n",
      "\n",
      " Epoch 82: \tTraining loss: 8.493 \tValidation loss: 4.987\n",
      "\n",
      " Epoch 83: \tTraining loss: 8.281 \tValidation loss: 4.938\n",
      "\n",
      " Epoch 84: \tTraining loss: 8.137 \tValidation loss: 5.030\n",
      "\n",
      " Epoch 85: \tTraining loss: 8.252 \tValidation loss: 4.975\n",
      "\n",
      " Epoch 86: \tTraining loss: 8.319 \tValidation loss: 4.967\n",
      "\n",
      " Epoch 87: \tTraining loss: 8.153 \tValidation loss: 5.321\n",
      "\n",
      " Epoch 88: \tTraining loss: 8.354 \tValidation loss: 5.260\n",
      "\n",
      " Epoch 89: \tTraining loss: 8.192 \tValidation loss: 5.028\n",
      "\n",
      " Epoch 90: \tTraining loss: 8.405 \tValidation loss: 5.264\n",
      "\n",
      " Epoch 91: \tTraining loss: 8.198 \tValidation loss: 5.078\n",
      "\n",
      " Epoch 92: \tTraining loss: 8.474 \tValidation loss: 4.892\n",
      "\n",
      " Epoch 93: \tTraining loss: 8.281 \tValidation loss: 4.955\n",
      "\n",
      " Epoch 94: \tTraining loss: 8.403 \tValidation loss: 4.886\n",
      "\n",
      " Epoch 95: \tTraining loss: 8.554 \tValidation loss: 4.926\n",
      "\n",
      " Epoch 96: \tTraining loss: 8.408 \tValidation loss: 4.927\n",
      "\n",
      " Epoch 97: \tTraining loss: 8.053 \tValidation loss: 5.057\n",
      "\n",
      " Epoch 98: \tTraining loss: 8.488 \tValidation loss: 4.963\n",
      "\n",
      " Epoch 99: \tTraining loss: 7.976 \tValidation loss: 5.022\n",
      "\n",
      " Epoch 100: \tTraining loss: 8.544 \tValidation loss: 4.896\n",
      "Evaluation on test set\n",
      "R2: 0.8765408992767334\n"
     ]
    }
   ],
   "source": [
    "print(\"Question 8.2:\")\n",
    "class MLP82(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.output = nn.Linear(hidden_dims, output_dims)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.linear1(x))\n",
    "        out = self.output(x)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "model = MLP82(input_dims=X_train.shape[1], hidden_dims=64, output_dims=1).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "epochs = 100\n",
    "train_losses, val_losses = [], []\n",
    "train_r2, val_r2 = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    train_target, train_predict = [], []\n",
    "    val_target, val_predict = [], []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for X_samples, y_samples in train_loader:\n",
    "        X_samples = X_samples.to(device)\n",
    "        y_samples = y_samples.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(X_samples)\n",
    "        loss = criterion(outputs, y_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_predict += outputs.tolist()\n",
    "        train_target += y_samples.tolist()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_r2.append(r_squared(train_target, train_predict))\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_samples, y_samples in val_loader:\n",
    "            X_samples = X_samples.to(device)\n",
    "            y_samples = y_samples.to(device)\n",
    "            \n",
    "            outputs = model(X_samples)\n",
    "            loss = criterion(outputs, y_samples)\n",
    "            \n",
    "            val_predict += outputs.tolist()\n",
    "            val_target += y_samples.tolist()\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_r2.append(r_squared(val_target, val_predict))\n",
    "    print(f\"\\n Epoch {epoch + 1}: \\tTraining loss: {train_loss:.3f} \\tValidation loss: {val_loss:.3f}\")\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test.to(device))\n",
    "    test_set_r2 = r_squared(y_test, y_pred)\n",
    "    print(\"Evaluation on test set\")\n",
    "    print(f\"R2: {test_set_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 8.3:\n",
      "\n",
      " Epoch 1: \tTraining loss: 300.393 \tValidation loss: 16.904\n",
      "\n",
      " Epoch 2: \tTraining loss: 17.776 \tValidation loss: 13.384\n",
      "\n",
      " Epoch 3: \tTraining loss: 15.651 \tValidation loss: 11.331\n",
      "\n",
      " Epoch 4: \tTraining loss: 14.712 \tValidation loss: 7.728\n",
      "\n",
      " Epoch 5: \tTraining loss: 14.013 \tValidation loss: 8.078\n",
      "\n",
      " Epoch 6: \tTraining loss: 12.617 \tValidation loss: 7.035\n",
      "\n",
      " Epoch 7: \tTraining loss: 12.241 \tValidation loss: 12.890\n",
      "\n",
      " Epoch 8: \tTraining loss: 11.020 \tValidation loss: 6.790\n",
      "\n",
      " Epoch 9: \tTraining loss: 11.681 \tValidation loss: 7.783\n",
      "\n",
      " Epoch 10: \tTraining loss: 10.476 \tValidation loss: 7.573\n",
      "\n",
      " Epoch 11: \tTraining loss: 10.037 \tValidation loss: 7.166\n",
      "\n",
      " Epoch 12: \tTraining loss: 10.924 \tValidation loss: 7.408\n",
      "\n",
      " Epoch 13: \tTraining loss: 9.410 \tValidation loss: 6.846\n",
      "\n",
      " Epoch 14: \tTraining loss: 9.591 \tValidation loss: 5.411\n",
      "\n",
      " Epoch 15: \tTraining loss: 10.093 \tValidation loss: 7.237\n",
      "\n",
      " Epoch 16: \tTraining loss: 9.192 \tValidation loss: 6.043\n",
      "\n",
      " Epoch 17: \tTraining loss: 9.894 \tValidation loss: 8.277\n",
      "\n",
      " Epoch 18: \tTraining loss: 9.819 \tValidation loss: 5.571\n",
      "\n",
      " Epoch 19: \tTraining loss: 8.576 \tValidation loss: 5.844\n",
      "\n",
      " Epoch 20: \tTraining loss: 9.078 \tValidation loss: 10.039\n",
      "\n",
      " Epoch 21: \tTraining loss: 9.887 \tValidation loss: 6.569\n",
      "\n",
      " Epoch 22: \tTraining loss: 8.626 \tValidation loss: 6.192\n",
      "\n",
      " Epoch 23: \tTraining loss: 8.563 \tValidation loss: 5.866\n",
      "\n",
      " Epoch 24: \tTraining loss: 8.231 \tValidation loss: 5.776\n",
      "\n",
      " Epoch 25: \tTraining loss: 8.253 \tValidation loss: 4.861\n",
      "\n",
      " Epoch 26: \tTraining loss: 8.316 \tValidation loss: 7.243\n",
      "\n",
      " Epoch 27: \tTraining loss: 8.632 \tValidation loss: 5.068\n",
      "\n",
      " Epoch 28: \tTraining loss: 7.896 \tValidation loss: 4.413\n",
      "\n",
      " Epoch 29: \tTraining loss: 7.694 \tValidation loss: 4.962\n",
      "\n",
      " Epoch 30: \tTraining loss: 8.467 \tValidation loss: 7.091\n",
      "\n",
      " Epoch 31: \tTraining loss: 8.619 \tValidation loss: 4.340\n",
      "\n",
      " Epoch 32: \tTraining loss: 7.776 \tValidation loss: 5.438\n",
      "\n",
      " Epoch 33: \tTraining loss: 7.898 \tValidation loss: 4.894\n",
      "\n",
      " Epoch 34: \tTraining loss: 7.867 \tValidation loss: 4.974\n",
      "\n",
      " Epoch 35: \tTraining loss: 7.455 \tValidation loss: 4.739\n",
      "\n",
      " Epoch 36: \tTraining loss: 7.596 \tValidation loss: 4.878\n",
      "\n",
      " Epoch 37: \tTraining loss: 7.541 \tValidation loss: 6.474\n",
      "\n",
      " Epoch 38: \tTraining loss: 7.356 \tValidation loss: 5.379\n",
      "\n",
      " Epoch 39: \tTraining loss: 7.210 \tValidation loss: 4.593\n",
      "\n",
      " Epoch 40: \tTraining loss: 7.334 \tValidation loss: 4.661\n",
      "\n",
      " Epoch 41: \tTraining loss: 7.200 \tValidation loss: 5.379\n",
      "\n",
      " Epoch 42: \tTraining loss: 7.214 \tValidation loss: 5.399\n",
      "\n",
      " Epoch 43: \tTraining loss: 7.155 \tValidation loss: 5.108\n",
      "\n",
      " Epoch 44: \tTraining loss: 7.331 \tValidation loss: 5.882\n",
      "\n",
      " Epoch 45: \tTraining loss: 6.824 \tValidation loss: 4.760\n",
      "\n",
      " Epoch 46: \tTraining loss: 7.073 \tValidation loss: 5.915\n",
      "\n",
      " Epoch 47: \tTraining loss: 6.978 \tValidation loss: 4.941\n",
      "\n",
      " Epoch 48: \tTraining loss: 7.374 \tValidation loss: 5.914\n",
      "\n",
      " Epoch 49: \tTraining loss: 6.765 \tValidation loss: 5.288\n",
      "\n",
      " Epoch 50: \tTraining loss: 7.232 \tValidation loss: 4.844\n",
      "\n",
      " Epoch 51: \tTraining loss: 7.140 \tValidation loss: 5.287\n",
      "\n",
      " Epoch 52: \tTraining loss: 7.438 \tValidation loss: 8.790\n",
      "\n",
      " Epoch 53: \tTraining loss: 7.332 \tValidation loss: 5.601\n",
      "\n",
      " Epoch 54: \tTraining loss: 7.467 \tValidation loss: 9.238\n",
      "\n",
      " Epoch 55: \tTraining loss: 7.267 \tValidation loss: 5.322\n",
      "\n",
      " Epoch 56: \tTraining loss: 6.493 \tValidation loss: 5.796\n",
      "\n",
      " Epoch 57: \tTraining loss: 7.005 \tValidation loss: 5.434\n",
      "\n",
      " Epoch 58: \tTraining loss: 6.950 \tValidation loss: 5.037\n",
      "\n",
      " Epoch 59: \tTraining loss: 6.327 \tValidation loss: 4.873\n",
      "\n",
      " Epoch 60: \tTraining loss: 6.575 \tValidation loss: 5.359\n",
      "\n",
      " Epoch 61: \tTraining loss: 6.995 \tValidation loss: 5.043\n",
      "\n",
      " Epoch 62: \tTraining loss: 6.511 \tValidation loss: 5.159\n",
      "\n",
      " Epoch 63: \tTraining loss: 6.338 \tValidation loss: 4.870\n",
      "\n",
      " Epoch 64: \tTraining loss: 6.707 \tValidation loss: 5.427\n",
      "\n",
      " Epoch 65: \tTraining loss: 7.770 \tValidation loss: 7.184\n",
      "\n",
      " Epoch 66: \tTraining loss: 6.858 \tValidation loss: 5.448\n",
      "\n",
      " Epoch 67: \tTraining loss: 6.145 \tValidation loss: 5.524\n",
      "\n",
      " Epoch 68: \tTraining loss: 6.533 \tValidation loss: 5.471\n",
      "\n",
      " Epoch 69: \tTraining loss: 6.656 \tValidation loss: 5.160\n",
      "\n",
      " Epoch 70: \tTraining loss: 6.381 \tValidation loss: 4.869\n",
      "\n",
      " Epoch 71: \tTraining loss: 6.784 \tValidation loss: 6.025\n",
      "\n",
      " Epoch 72: \tTraining loss: 6.518 \tValidation loss: 5.464\n",
      "\n",
      " Epoch 73: \tTraining loss: 6.270 \tValidation loss: 5.132\n",
      "\n",
      " Epoch 74: \tTraining loss: 6.503 \tValidation loss: 5.845\n",
      "\n",
      " Epoch 75: \tTraining loss: 6.088 \tValidation loss: 5.918\n",
      "\n",
      " Epoch 76: \tTraining loss: 6.478 \tValidation loss: 5.671\n",
      "\n",
      " Epoch 77: \tTraining loss: 6.515 \tValidation loss: 7.015\n",
      "\n",
      " Epoch 78: \tTraining loss: 6.939 \tValidation loss: 5.727\n",
      "\n",
      " Epoch 79: \tTraining loss: 7.230 \tValidation loss: 9.345\n",
      "\n",
      " Epoch 80: \tTraining loss: 6.710 \tValidation loss: 5.514\n",
      "\n",
      " Epoch 81: \tTraining loss: 6.350 \tValidation loss: 5.215\n",
      "\n",
      " Epoch 82: \tTraining loss: 6.241 \tValidation loss: 6.010\n",
      "\n",
      " Epoch 83: \tTraining loss: 6.145 \tValidation loss: 5.421\n",
      "\n",
      " Epoch 84: \tTraining loss: 6.054 \tValidation loss: 5.428\n",
      "\n",
      " Epoch 85: \tTraining loss: 6.437 \tValidation loss: 5.558\n",
      "\n",
      " Epoch 86: \tTraining loss: 6.175 \tValidation loss: 5.027\n",
      "\n",
      " Epoch 87: \tTraining loss: 5.903 \tValidation loss: 5.379\n",
      "\n",
      " Epoch 88: \tTraining loss: 5.958 \tValidation loss: 5.168\n",
      "\n",
      " Epoch 89: \tTraining loss: 5.722 \tValidation loss: 5.870\n",
      "\n",
      " Epoch 90: \tTraining loss: 5.939 \tValidation loss: 5.208\n",
      "\n",
      " Epoch 91: \tTraining loss: 6.290 \tValidation loss: 5.705\n",
      "\n",
      " Epoch 92: \tTraining loss: 5.791 \tValidation loss: 6.075\n",
      "\n",
      " Epoch 93: \tTraining loss: 5.884 \tValidation loss: 5.469\n",
      "\n",
      " Epoch 94: \tTraining loss: 5.857 \tValidation loss: 5.152\n",
      "\n",
      " Epoch 95: \tTraining loss: 6.237 \tValidation loss: 5.327\n",
      "\n",
      " Epoch 96: \tTraining loss: 5.851 \tValidation loss: 5.209\n",
      "\n",
      " Epoch 97: \tTraining loss: 6.333 \tValidation loss: 6.156\n",
      "\n",
      " Epoch 98: \tTraining loss: 5.761 \tValidation loss: 7.308\n",
      "\n",
      " Epoch 99: \tTraining loss: 6.154 \tValidation loss: 5.242\n",
      "\n",
      " Epoch 100: \tTraining loss: 6.011 \tValidation loss: 5.009\n",
      "Evaluation on test set\n",
      "R2: 0.893171489238739\n"
     ]
    }
   ],
   "source": [
    "print(\"Question 8.3:\")\n",
    "\n",
    "class MLP83(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.output = nn.Linear(hidden_dims, output_dims)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.tanh(x)\n",
    "        out = self.output(x)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "model = MLP83(\n",
    "    input_dims=X_train.shape[1], \n",
    "    hidden_dims=64, \n",
    "    output_dims=1\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "epochs = 100\n",
    "train_losses, val_losses = [], []\n",
    "train_r2, val_r2 = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    train_target, train_predict = [], []\n",
    "    val_target, val_predict = [], []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for X_samples, y_samples in train_loader:\n",
    "        X_samples = X_samples.to(device)\n",
    "        y_samples = y_samples.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(X_samples)\n",
    "        loss = criterion(outputs, y_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_predict += outputs.tolist()\n",
    "        train_target += y_samples.tolist()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_r2.append(r_squared(train_target, train_predict))\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_samples, y_samples in val_loader:\n",
    "            X_samples = X_samples.to(device)\n",
    "            y_samples = y_samples.to(device)\n",
    "            \n",
    "            outputs = model(X_samples)\n",
    "            loss = criterion(outputs, y_samples)\n",
    "            \n",
    "            val_predict += outputs.tolist()\n",
    "            val_target += y_samples.tolist()\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_r2.append(r_squared(val_target, val_predict))\n",
    "    print(f\"\\n Epoch {epoch + 1}: \\tTraining loss: {train_loss:.3f} \\tValidation loss: {val_loss:.3f}\")\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test.to(device))\n",
    "    test_set_r2 = r_squared(y_test, y_pred)\n",
    "    print(\"Evaluation on test set\")\n",
    "    print(f\"R2: {test_set_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
